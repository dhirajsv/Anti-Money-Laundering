# -*- coding: utf-8 -*-
"""Anti-Money Laundering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lFsa5bt6tNg6kWtMHI0u6ZDsShjEhHn_
"""

!pip install pandas
!pip install networkx
!pip install torch
!pip install scikit-learn

!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html
!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html
!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html
!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html
!pip install torch-geometric

import pandas as pd
import networkx as nx
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data
import torch.optim as optim

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

data = pd.read_csv("/content/drive/MyDrive/MLGA/raw/LI-Small_Trans.csv")

G = nx.DiGraph()


for i, row in data.head(200).iterrows():
    G.add_edge(row['Account'], row['To Account'], amount=row['Amount Received'])
with open("/content/drive/MyDrive/MLGA/raw/LI-Small_Patterns.txt", "r") as file:
    lines = file.readlines()

    for line in lines:
        if line.startswith("BEGIN LAUNDERING ATTEMPT"):
            laundering_attempt = True
        elif line.startswith("END LAUNDERING ATTEMPT"):
            laundering_attempt = False
        elif laundering_attempt:
            transaction_info = line.split(",")
            if len(transaction_info) == 11:
                from_account = transaction_info[2]
                to_account = transaction_info[4]
                # Add nodes if they don't exist
                if from_account not in G:
                    G.add_node(from_account)
                if to_account not in G:
                    G.add_node(to_account)
                # Add edge with 'color' attribute
                G.add_edge(from_account, to_account, color='red')

# Visualize the graph
plt.figure(figsize=(20, 10))
pos = nx.spring_layout(G, seed=42)

# Draw edges
edge_colors = [G[u][v].get('color', 'gray') for u, v in G.edges()]
nx.draw_networkx_edges(G, pos, edgelist=G.edges(), edge_color=edge_colors)

# Draw nodes and labels
nx.draw_networkx_nodes(G, pos, node_size=200, node_color="skyblue")
nx.draw_networkx_labels(G, pos, font_size=8)

plt.title("Transaction Graph with Money Laundering Flags (First 200 lines)")
plt.show()

print(data.head())

print(data.dtypes)

print('Amount Received equals to Amount Paid:')
print(data['Amount Received'].equals(data['Amount Paid']))
print('Receiving Currency equals to Payment Currency:')
print(data['Receiving Currency'].equals(data['Payment Currency']))

# Transaction between different currencies
not_equal1 = data.loc[~(data['Amount Received'] == data['Amount Paid'])]
not_equal2 = data.loc[~(data['Receiving Currency'] == data['Payment Currency'])]
print(not_equal1)
print('---------------------------------------------------------------------------')
print(not_equal2)

Laundering = data[data['Is Laundering']==1]
Non_Laundering = data[data['Is Laundering']==0]
print(len(Laundering))
print(len(Non_Laundering))

same_currency_transactionsL = Laundering[Laundering['Receiving Currency'] == Laundering['Payment Currency']]
same_currencyL = same_currency_transactionsL.groupby('Payment Currency').size().reset_index(name='Transaction Count')

same_currencyL.describe()

same_currencyL.head()

import seaborn as sns
plt.figure(figsize=(18, 6))
custom_palette = sns.color_palette("husl", len(same_currencyL['Payment Currency'].unique()))

sns.barplot(x='Payment Currency', y='Transaction Count', data=same_currencyL,color="cyan")
plt.title('Laundering transaction Counts by Currency')
plt.xlabel('Currency')
plt.ylabel('Transaction Count')
plt.show()

df=data
import sklearn.preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing

print(df.columns)

def df_label_encoder(df, columns):
        le = preprocessing.LabelEncoder()
        for i in columns:
            df[i] = le.fit_transform(df[i].astype(str))
        return df

def preprocess(df):
    df = df_label_encoder(df,['Payment Format', 'Payment Currency', 'Receiving Currency'])
    df['Timestamp'] = pd.to_datetime(df['Timestamp'])
    df['Timestamp'] = df['Timestamp'].apply(lambda x: x.value)
    df['Timestamp'] = (df['Timestamp']-df['Timestamp'].min())/(df['Timestamp'].max()-df['Timestamp'].min())

    df['Account'] = df['From Bank'].astype(str) + '_' + df['Account']
    df['To Account'] = df['To Bank'].astype(str) + '_' + df['To Account']
    df = df.sort_values(by=['Account'])
    receiving_df = df[['To Account', 'Amount Received', 'Receiving Currency']]
    paying_df = df[['Account', 'Amount Paid', 'Payment Currency']]
    receiving_df = receiving_df.rename({'To Account': 'Account'}, axis=1)
    currency_ls = sorted(df['Receiving Currency'].unique())

    return df, receiving_df, paying_df, currency_ls

df, receiving_df, paying_df, currency_ls = preprocess(df = df)
print(df.head())

def get_all_account(df):
        ldf = df[['Account', 'From Bank']]
        rdf = df[['To Account', 'To Bank']]
        suspicious = df[df['Is Laundering']==1]
        s1 = suspicious[['Account', 'Is Laundering']]
        s2 = suspicious[['To Account', 'Is Laundering']]
        s2 = s2.rename({'To Account': 'Account'}, axis=1)
        suspicious = pd.concat([s1, s2], join='outer')
        suspicious = suspicious.drop_duplicates()

        ldf = ldf.rename({'From Bank': 'Bank'}, axis=1)
        rdf = rdf.rename({'To Account': 'Account', 'To Bank': 'Bank'}, axis=1)
        df = pd.concat([ldf, rdf], join='outer')
        df = df.drop_duplicates()

        df['Is Laundering'] = 0
        df.set_index('Account', inplace=True)
        df.update(suspicious.set_index('Account'))
        df = df.reset_index()
        return df
accounts = get_all_account(df)

def paid_currency_aggregate(currency_ls, paying_df, accounts):
        for i in currency_ls:
            temp = paying_df[paying_df['Payment Currency'] == i]
            accounts['avg paid '+str(i)] = temp['Amount Paid'].groupby(temp['Account']).transform('mean')
        return accounts

def received_currency_aggregate(currency_ls, receiving_df, accounts):
    for i in currency_ls:
        temp = receiving_df[receiving_df['Receiving Currency'] == i]
        accounts['avg received '+str(i)] = temp['Amount Received'].groupby(temp['Account']).transform('mean')
    accounts = accounts.fillna(0)
    return accounts
def get_node_attr(currency_ls, paying_df,receiving_df, accounts):
        node_df = paid_currency_aggregate(currency_ls, paying_df, accounts)
        node_df = received_currency_aggregate(currency_ls, receiving_df, node_df)
        node_label = torch.from_numpy(node_df['Is Laundering'].values).to(torch.float)
        node_df = node_df.drop(['Account', 'Is Laundering'], axis=1)
        node_df = df_label_encoder(node_df,['Bank'])
#         node_df = torch.from_numpy(node_df.values).to(torch.float)  # comment for visualization
        return node_df, node_label
node_df, node_label = get_node_attr(currency_ls, paying_df,receiving_df, accounts)
print(node_df.head())

def get_edge_df(accounts, df):
        accounts = accounts.reset_index(drop=True)
        accounts['ID'] = accounts.index
        mapping_dict = dict(zip(accounts['Account'], accounts['ID']))
        df['From'] = df['Account'].map(mapping_dict)
        df['To'] = df['To Account'].map(mapping_dict)
        df = df.drop(['Account', 'To Account', 'From Bank', 'To Bank'], axis=1)

        edge_index = torch.stack([torch.from_numpy(df['From'].values), torch.from_numpy(df['To'].values)], dim=0)

        df = df.drop(['Is Laundering', 'From', 'To'], axis=1)

        edge_attr = df  # for visualization
        return edge_attr, edge_index
edge_attr, edge_index = get_edge_df(accounts, df)
print(edge_attr.head())

print(edge_index)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric.transforms as T
from torch_geometric.nn import GATConv, Linear

class GAT(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads, dropout=0.6)
        self.conv2 = GATConv(hidden_channels * heads, int(hidden_channels/4), heads=1, concat=False, dropout=0.6)
        self.lin = Linear(int(hidden_channels/4), out_channels)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, edge_index, edge_attr):
        x = F.dropout(x, p=0.6, training=self.training)
        x = F.elu(self.conv1(x, edge_index, edge_attr))
        x = F.dropout(x, p=0.6, training=self.training)
        x = F.elu(self.conv2(x, edge_index, edge_attr))
        x = self.lin(x)
        x = self.sigmoid(x)

        return x

from torch_geometric.data import InMemoryDataset
from typing import Callable, Optional

class AMLtoGraph(InMemoryDataset):

    def __init__(self, root: str, edge_window_size: int = 10,
                 transform: Optional[Callable] = None,
                 pre_transform: Optional[Callable] = None):
        self.edge_window_size = edge_window_size
        super().__init__(root, transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self) -> str:
        return 'LI-Small_Trans.csv'

    @property
    def processed_file_names(self) -> str:
        return 'data.pt'

    @property
    def num_nodes(self) -> int:
        return self._data.edge_index.max().item() + 1

    def df_label_encoder(self, df, columns):
        le = preprocessing.LabelEncoder()
        for i in columns:
            df[i] = le.fit_transform(df[i].astype(str))
        return df


    def preprocess(self, df):
        df = self.df_label_encoder(df,['Payment Format', 'Payment Currency', 'Receiving Currency'])
        df['Timestamp'] = pd.to_datetime(df['Timestamp'])
        df['Timestamp'] = df['Timestamp'].apply(lambda x: x.value)
        df['Timestamp'] = (df['Timestamp']-df['Timestamp'].min())/(df['Timestamp'].max()-df['Timestamp'].min())

        df['Account'] = df['From Bank'].astype(str) + '_' + df['Account']
        df['To Account'] = df['To Bank'].astype(str) + '_' + df['To Account']
        df = df.sort_values(by=['Account'])
        receiving_df = df[['To Account', 'Amount Received', 'Receiving Currency']]
        paying_df = df[['Account', 'Amount Paid', 'Payment Currency']]
        receiving_df = receiving_df.rename({'To Account': 'Account'}, axis=1)
        currency_ls = sorted(df['Receiving Currency'].unique())

        return df, receiving_df, paying_df, currency_ls

    def get_all_account(self, df):
        ldf = df[['Account', 'From Bank']]
        rdf = df[['To Account', 'To Bank']]
        suspicious = df[df['Is Laundering']==1]
        s1 = suspicious[['Account', 'Is Laundering']]
        s2 = suspicious[['To Account', 'Is Laundering']]
        s2 = s2.rename({'To Account': 'Account'}, axis=1)
        suspicious = pd.concat([s1, s2], join='outer')
        suspicious = suspicious.drop_duplicates()

        ldf = ldf.rename({'From Bank': 'Bank'}, axis=1)
        rdf = rdf.rename({'To Account': 'Account', 'To Bank': 'Bank'}, axis=1)
        df = pd.concat([ldf, rdf], join='outer')
        df = df.drop_duplicates()

        df['Is Laundering'] = 0
        df.set_index('Account', inplace=True)
        df.update(suspicious.set_index('Account'))
        df = df.reset_index()
        return df

    def paid_currency_aggregate(self, currency_ls, paying_df, accounts):
        for i in currency_ls:
            temp = paying_df[paying_df['Payment Currency'] == i]
            accounts['avg paid '+str(i)] = temp['Amount Paid'].groupby(temp['Account']).transform('mean')
        return accounts

    def received_currency_aggregate(self, currency_ls, receiving_df, accounts):
        for i in currency_ls:
            temp = receiving_df[receiving_df['Receiving Currency'] == i]
            accounts['avg received '+str(i)] = temp['Amount Received'].groupby(temp['Account']).transform('mean')
        accounts = accounts.fillna(0)
        return accounts

    def get_edge_df(self, accounts, df):
        accounts = accounts.reset_index(drop=True)
        accounts['ID'] = accounts.index
        mapping_dict = dict(zip(accounts['Account'], accounts['ID']))
        df['From'] = df['Account'].map(mapping_dict)
        df['To'] = df['To Account'].map(mapping_dict)
        df = df.drop(['Account', 'To Account', 'From Bank', 'To Bank'], axis=1)

        edge_index = torch.stack([torch.from_numpy(df['From'].values), torch.from_numpy(df['To'].values)], dim=0)

        df = df.drop(['Is Laundering', 'From', 'To'], axis=1)

        edge_attr = torch.from_numpy(df.values).to(torch.float)
        return edge_attr, edge_index

    def get_node_attr(self, currency_ls, paying_df,receiving_df, accounts):
        node_df = self.paid_currency_aggregate(currency_ls, paying_df, accounts)
        node_df = self.received_currency_aggregate(currency_ls, receiving_df, node_df)
        node_label = torch.from_numpy(node_df['Is Laundering'].values).to(torch.float)
        node_df = node_df.drop(['Account', 'Is Laundering'], axis=1)
        node_df = self.df_label_encoder(node_df,['Bank'])
        node_df = torch.from_numpy(node_df.values).to(torch.float)
        return node_df, node_label

    def process(self):
        df = pd.read_csv(self.raw_paths[0])
        df, receiving_df, paying_df, currency_ls = self.preprocess(df)
        accounts = self.get_all_account(df)
        node_attr, node_label = self.get_node_attr(currency_ls, paying_df,receiving_df, accounts)
        edge_attr, edge_index = self.get_edge_df(accounts, df)

        data = Data(x=node_attr,
                    edge_index=edge_index,
                    y=node_label,
                    edge_attr=edge_attr
                    )

        data_list = [data]
        if self.pre_filter is not None:
            data_list = [d for d in data_list if self.pre_filter(d)]

        if self.pre_transform is not None:
            data_list = [self.pre_transform(d) for d in data_list]

        data, slices = self.collate(data_list)
        torch.save((data, slices), self.processed_paths[0])

from torch_geometric.transforms import RandomNodeSplit
from torch_geometric.loader import NeighborLoader

dataset = AMLtoGraph('/content/drive/MyDrive/MLGA')
data = dataset[0]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = GAT(in_channels=data.num_features, hidden_channels=16, out_channels=1, heads=8)
model = model.to(device)


criterion = torch.nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)

# Split the data
split = RandomNodeSplit(split='train_rest', num_val=0.1, num_test=0)
data = split(data)

# Define the data loaders
train_loader = NeighborLoader(data, num_neighbors=[30] * 2, batch_size=256, input_nodes=data.train_mask)
test_loader = NeighborLoader(data, num_neighbors=[30] * 2, batch_size=256, input_nodes=data.val_mask)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    total_loss = 0
    model.train()
    for data in train_loader:
        optimizer.zero_grad()
        data.to(device)
        pred = model(data.x, data.edge_index, data.edge_attr)
        ground_truth = data.y
        loss = criterion(pred, ground_truth.unsqueeze(1))
        loss.backward()
        optimizer.step()
        total_loss += float(loss)
    if epoch % 2 == 0:
        print(f"Epoch: {epoch:03d}, Loss: {total_loss:.4f}")
        model.eval()
        acc = 0
        total = 0
        with torch.no_grad():
            for test_data in test_loader:
                test_data.to(device)
                pred = model(test_data.x, test_data.edge_index, test_data.edge_attr)
                ground_truth = test_data.y
                correct = (pred == ground_truth.unsqueeze(1)).sum().item()
                total += len(ground_truth)
                acc += correct
            acc = acc / total
            print('accuracy:', acc)